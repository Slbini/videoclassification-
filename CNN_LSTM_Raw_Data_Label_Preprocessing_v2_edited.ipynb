{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Slbini/videoclassification-/blob/main/CNN_LSTM_Raw_Data_Label_Preprocessing_v2_edited.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT-eGFinpr70"
      },
      "source": [
        "# <span style=\"color:orange;\">Raw Data-Label Preprocessing for Training Data v2</span>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Google Drive 마운트\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 마운트된 경로 확인\n",
        "print(\"Google Drive is mounted at '/content/drive'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdWr2qQ36Qhl",
        "outputId": "1672d149-d4f7-4ba3-dbdd-1e9e9487917e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive is mounted at '/content/drive'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCtTKzXMpr73"
      },
      "source": [
        "## VideoDataset_Unified Class (extends Dataset)\n",
        " - constructor는 raw data의 경로와 대응되는 label의 경로를 파라미터로 받습니다. 그러면 이에 대응되는 dataset이 만들어집니다.\n",
        " - 데이터를 getitem 요청이 들어올 때마다 동적으로 구성하여 메모리 효율을 개선했습니다. 메모리 소모량은 영상 데이터의 크기와 거의 같습니다.\n",
        " - 여기서 각 data는 (video, label) 꼴이며, label은 {'S1': 0, 'S2': 1, 'S3': 2, 'S4': 3, 'S5': 4, 'S10': 5, 'S20': 6}에 따라 mapping된 정수이고, video는 shape가 (10, 40, 60)인 float tensor(pytorch)입니다.\n",
        " - 전처리 과정은 최소화하는 게 목적이었으므로 이미지 resizing조차 하지 않았습니다. 이 부분은 model forward에서 interpolation 등으로 처리해야 할 것 같습니다.\n",
        " - VideoDataset_Unified의 생성자에 transform이 추가되었습니다. Usage에서는 데이터를 64x64로 만든 뒤 -1~1로 정규화를 수행하고 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2025-02-25 수정사항\n",
        " - 전체 데이터를 랜덤 분할하여 train, validation, test 데이터 셋을 각각 독립적으로 구성하도록 수정했습니다 데이터가 겹치지 않으므로 validation 및 test의 평가 정확도가 과대 평가되지 않습니다.\n",
        " -train_loss 와 val_loss가 동일한 방식으로 평균을 계산하도록 수정되었습니다. 각 배치의 손실을 합산한 후, 전체 배치 수로 나누어 최종 손실 값을 계산합니다."
      ],
      "metadata": {
        "id": "PQ9ESzb8MVqY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Hi5QRMhzpr73"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import glob\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pYYuyOWIpr74"
      },
      "outputs": [],
      "source": [
        "class VideoDataset_Unified(Dataset):\n",
        "    def __load_label(self, **kwargs):\n",
        "        label_map = kwargs['label_map'] if 'label_map' in kwargs else {'S1': 0, 'S2': 1, 'S3': 2, 'S4': 3, 'S5': 4, 'S10': 5, 'S20': 6}\n",
        "\n",
        "        raw_label_dat = json.load(open(self.label_path, encoding='utf-8'))\n",
        "        for key in raw_label_dat.keys():\n",
        "            # 'frame_0000xx' -> int('0000xx') = xx\n",
        "            frame_index = int(key.split('_')[1])\n",
        "\n",
        "            # 'Sn(description)' -> 'Sn'\n",
        "            #frame_labels = tuple(map(lambda s: label_map[s.split('(')[0]], raw_label_dat[key]['labels']))\n",
        "            if (len(raw_label_dat[key]['labels']) == 0): continue\n",
        "            frame_label = label_map[raw_label_dat[key]['labels'][0].split('(')[0]]\n",
        "            self.label_dict[frame_index] = frame_label\n",
        "\n",
        "    def __load_data(self, **kwargs):\n",
        "        FPS = 20\n",
        "        image_hw = kwargs['image_hw'] if 'image_hw' in kwargs else (64, 64)\n",
        "        frames_before_sec = kwargs['frames_before_sec'] if 'frames_before_sec' in kwargs else (0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8)\n",
        "        use_init_frames = kwargs['use_init_frames'] if 'use_init_frames' in kwargs else False\n",
        "\n",
        "        self.frames = []\n",
        "        try:\n",
        "            with open(self.raw_path, 'r') as file:\n",
        "                frames_raw_text = file.read()\n",
        "                frames_raw = re.split(r't:', frames_raw_text)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Unable to open file {self.raw_path}\")\n",
        "            return\n",
        "\n",
        "        # 각 줄을 읽어 이미지 데이터로 변환\n",
        "        for frame_index, frame in enumerate(frames_raw):\n",
        "            thermal_data_values = [int(value) for value in re.findall(r'\\d+', frame)]\n",
        "\n",
        "            # to avoid metadata reading\n",
        "            if(thermal_data_values[0] < 20):\n",
        "                thermal_data_values = thermal_data_values[3:]\n",
        "\n",
        "            if len(thermal_data_values) >= 2400:\n",
        "                # 1d data to 40x60 tensor (flipped)\n",
        "                if(self.transform is not None):\n",
        "                    frame_4060 = self.transform(torch.tensor(thermal_data_values[:2400], dtype=torch.float32).reshape(1, 40, 60).flip((-1,)))\n",
        "                else:\n",
        "                    frame_4060 = torch.tensor(thermal_data_values[:2400], dtype=torch.float32).reshape(1, 40, 60).flip((-1,))\n",
        "                # 40x60 -> 64x64 (not implemented)\n",
        "                #frame_resized = nn.functional.interpolate(frame_4060.reshape(1, 1, 40, 60), image_hw, mode='bilinear', align_corners=False).squeeze()\n",
        "                self.frames.append(frame_4060)\n",
        "\n",
        "                if(self.test_mode or frame_index in self.label_dict.keys()):\n",
        "                    if(not use_init_frames and frame_index < round(frames_before_sec[-1] * FPS)):\n",
        "                        continue\n",
        "                    self.data_info.append({\n",
        "                        'img_idx': tuple(max(frame_index - round(t * FPS), 0) for t in reversed(frames_before_sec)),\n",
        "                        'label': self.label_dict[frame_index] if not self.test_mode else -1\n",
        "                    })\n",
        "\n",
        "    def __init__(self, raw_path, label_path, transform, test_mode=False, **kwargs):\n",
        "        self.test_mode = test_mode\n",
        "        self.raw_path = raw_path\n",
        "        self.label_path = label_path\n",
        "\n",
        "        self.label_dict = {}\n",
        "\n",
        "        self.frames = []\n",
        "        self.data_info = []\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        if(not self.test_mode):\n",
        "            self.__load_label(**kwargs)\n",
        "        self.__load_data(**kwargs)\n",
        "\n",
        "        print(f\"Data at raw: {raw_path}, label: {label_path} loaded.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video = torch.stack(tuple(self.frames[i] for i in self.data_info[idx]['img_idx']))\n",
        "        label = self.data_info[idx]['label']\n",
        "\n",
        "        return video, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zweFfqN9pr76"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7Bfwmripr76"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def visualize_all_frames(dat, idx=0):\n",
        "    clear_output(wait=True)\n",
        "    video=dat[0]\n",
        "    label=dat[1]\n",
        "\n",
        "    num_frames = video.shape[0]\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        # not working (beacuse of normalization)\n",
        "        min_temp, max_temp = 2900, 3050\n",
        "        frame = np.clip(video[i].view(64, 64).numpy(), min_temp, max_temp)\n",
        "        frame = (frame - min_temp) / (max_temp - min_temp) * 255\n",
        "\n",
        "        plt.subplot(1, num_frames, i + 1)\n",
        "        plt.imshow(frame, cmap=\"gray\")\n",
        "        plt.title(f\"Frame {i+1}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.suptitle(f\"All Frames of Video Index {idx}, Label: {label}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for i in range(len(dataset) // 10):\n",
        "    visualize_all_frames(dataset[i*10], idx=i*10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRa6vX1Tpr76"
      },
      "source": [
        "## Training Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dMo2c8Bwpr76"
      },
      "outputs": [],
      "source": [
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "        # CNN Layers\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 64 -> 16\n",
        "\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 16 -> 4\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 4 -> 1\n",
        "        )\n",
        "\n",
        "        # LSTM Layer\n",
        "        self.lstm = nn.LSTM(input_size=32, hidden_size=128, num_layers=2, batch_first=True)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, c, h, w = x.shape # 4, 20, 1, 64, 64\n",
        "        x = x.view(batch_size * seq_len, c, h, w)\n",
        "        x = self.cnn(x)\n",
        "        x = x.view(batch_size, seq_len, -1)  # (batch_size, seq_len, feature_dim) # 4 x 20 x 32\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        x = self.fc(hidden[-1])  # 마지막 LSTM layer의 hidden state 사용\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CIIcXiRwpr77"
      },
      "outputs": [],
      "source": [
        "# 학습 함수\n",
        "##################################################\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=100):\n",
        "##################################################\n",
        "    best_model = None\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct_preds, total_preds = 0, 0\n",
        "        num_batches = len(train_loader)\n",
        "\n",
        "        for videos, labels in train_loader:\n",
        "            videos, labels = videos.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(videos)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # 정확도 계산\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct_preds += (preds == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "        train_loss /= num_batches\n",
        "        train_acc = correct_preds / total_preds\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_preds, total_preds = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for videos, labels in val_loader:\n",
        "                videos, labels = videos.to(device), labels.to(device)\n",
        "                outputs = model(videos)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # 정확도 계산\n",
        "                _, preds = torch.max(outputs, dim=1)\n",
        "                correct_preds += (preds == labels).sum().item()\n",
        "                total_preds += labels.size(0)\n",
        "\n",
        "        val_loss /= num_batches\n",
        "        val_acc = correct_preds / total_preds\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "        # Best Model 저장\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model = model.state_dict()\n",
        "            ####################################################\n",
        "            torch.save(best_model, \"/content/drive/MyDrive/telecons/CNN_LSTM/best_model/best_model_CNNLSTM_20250225.pt\")\n",
        "            ####################################################\n",
        "            print(f\"Best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "    # 최종적으로 Best Model 로드\n",
        "    if best_model:\n",
        "        model.load_state_dict(best_model)\n",
        "        print(f\"Best model loaded with validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# 테스트 함수\n",
        "def test_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for videos, labels in test_loader:\n",
        "            videos = videos.to(device)\n",
        "            outputs = model(videos)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# 지정한 디렉토리 내에서 이름이 같은 JSON/TXT 파일 쌍 찾기\n",
        "data_dir = '/content/drive/MyDrive/telecons/data/20250221/ALL/'\n",
        "\n",
        "# glob을 이용해 .json과 .TXT 파일 리스트 얻기\n",
        "json_files = glob.glob(os.path.join(data_dir, \"*.json\"))\n",
        "txt_files = glob.glob(os.path.join(data_dir, \"*.TXT\"))\n",
        "\n",
        "# 파일명(확장자 제외)를 key로 매핑\n",
        "json_map = {os.path.splitext(os.path.basename(path))[0]: path for path in json_files}\n",
        "txt_map  = {os.path.splitext(os.path.basename(path))[0]: path for path in txt_files}\n",
        "\n",
        "# 두 매핑에서 공통된 파일명에 해당하는 쌍 생성 (raw_path: TXT, label_path: JSON)\n",
        "paths = []\n",
        "for key in json_map.keys():\n",
        "    if key in txt_map:\n",
        "        paths.append((txt_map[key], json_map[key]))\n",
        "\n",
        "print(f\"총 {len(paths)} 쌍의 파일을 찾았습니다.\")\n",
        "\n",
        "# transform 설정 (resize 및 normalize)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.Normalize((2868,), (34,))\n",
        "])\n",
        "\n",
        "# 데이터셋을 랜덤으로 train, val, test로 분할\n",
        "random.shuffle(paths)\n",
        "n_train = int(len(paths) * 0.8)\n",
        "n_val = int(len(paths) * 0.1)\n",
        "n_test = len(paths) - n_train - n_val\n",
        "\n",
        "train_paths = paths[:n_train]\n",
        "val_paths = paths[n_train:n_train + n_val]\n",
        "test_paths = paths[n_train + n_val:]\n",
        "\n",
        "# 분할된 데이터셋 정보 출력\n",
        "print(\"Train Set:\")\n",
        "for raw, label in train_paths:\n",
        "    print(f\"Raw: {raw}, Label: {label}\")\n",
        "print(\"\\nValidation Set:\")\n",
        "for raw, label in val_paths:\n",
        "    print(f\"Raw: {raw}, Label: {label}\")\n",
        "print(\"\\nTest Set:\")\n",
        "for raw, label in test_paths:\n",
        "    print(f\"Raw: {raw}, Label: {label}\")\n",
        "\n",
        "train_dataset_list, val_dataset_list, test_dataset_list = [], [], []\n",
        "for raw, label in train_paths:\n",
        "    dataset = VideoDataset_Unified(raw, label, transform)\n",
        "    train_dataset_list.append(dataset)\n",
        "for raw, label in val_paths:\n",
        "    dataset = VideoDataset_Unified(raw, label, transform)\n",
        "    val_dataset_list.append(dataset)\n",
        "for raw, label in test_paths:\n",
        "    dataset = VideoDataset_Unified(raw, label, transform)\n",
        "    test_dataset_list.append(dataset)\n",
        "\n",
        "train_dataset = ConcatDataset(train_dataset_list)\n",
        "val_dataset = ConcatDataset(val_dataset_list)\n",
        "test_dataset = ConcatDataset(test_dataset_list)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imlK60oK6aGP",
        "outputId": "83cf32e9-3703-4b38-cfeb-448326c6fd89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 26 쌍의 파일을 찾았습니다.\n",
            "Train Set:\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf4.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf4.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WF0_0.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/WF0_0.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf0.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf0.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WF1_0.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/WF1_0.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WF2_0.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/WF2_0.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf2.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf2.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CS20_3.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/CS20_3.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WF0_1.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/WF0_1.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CF2_0.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/CF2_0.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CS20_4.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/CS20_4.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WS20_3.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/WS20_3.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_2.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_2.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CF1_1.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/CF1_1.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250210.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250210.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_4.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_4.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CF1_0.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/CF1_0.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WF1_1.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/WF1_1.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf1.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf1.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf3.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf3.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_0.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_0.json\n",
            "\n",
            "Validation Set:\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WS20_4.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/WS20_4.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WF2_1.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/WF2_1.json\n",
            "\n",
            "Test Set:\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CF0_1.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/CF0_1.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CF0_0.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/CF0_0.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CF2_1.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/CF2_1.json\n",
            "Raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_1.TXT, Label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_1.json\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf4.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf4.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WF0_0.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/WF0_0.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf0.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf0.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WF1_0.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/WF1_0.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WF2_0.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/WF2_0.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf2.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf2.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CS20_3.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/CS20_3.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WF0_1.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/WF0_1.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CF2_0.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/CF2_0.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CS20_4.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/CS20_4.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WS20_3.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/WS20_3.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_2.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_2.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CF1_1.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/CF1_1.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250210.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250210.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_4.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_4.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CF1_0.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/CF1_0.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WF1_1.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/WF1_1.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf1.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf1.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf3.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250217_cf3.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_0.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_0.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WS20_4.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/WS20_4.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/WF2_1.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/WF2_1.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CF0_1.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/CF0_1.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CF0_0.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/CF0_0.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/CF2_1.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/CF2_1.json loaded.\n",
            "Data at raw: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_1.TXT, label: /content/drive/MyDrive/telecons/data/20250221/ALL/20250211_1.json loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9SGloo5pr77",
        "outputId": "d714cd3b-24db-493d-aa73-a16c28dece02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.4699, Train Accuracy: 0.8274, Val Loss: 0.6557, Val Accuracy: 0.5802\n",
            "Best model saved with validation accuracy: 0.5802\n",
            "Epoch 2, Train Loss: 0.1785, Train Accuracy: 0.9337, Val Loss: 0.8966, Val Accuracy: 0.5596\n",
            "Epoch 3, Train Loss: 0.1100, Train Accuracy: 0.9588, Val Loss: 1.0400, Val Accuracy: 0.5528\n",
            "Epoch 4, Train Loss: 0.0788, Train Accuracy: 0.9701, Val Loss: 1.2027, Val Accuracy: 0.6027\n",
            "Best model saved with validation accuracy: 0.6027\n",
            "Epoch 5, Train Loss: 0.0647, Train Accuracy: 0.9755, Val Loss: 0.9689, Val Accuracy: 0.5703\n",
            "Epoch 6, Train Loss: 0.0536, Train Accuracy: 0.9794, Val Loss: 1.1591, Val Accuracy: 0.5718\n",
            "Epoch 7, Train Loss: 0.0462, Train Accuracy: 0.9821, Val Loss: 1.2149, Val Accuracy: 0.5703\n",
            "Epoch 8, Train Loss: 0.0427, Train Accuracy: 0.9833, Val Loss: 1.1881, Val Accuracy: 0.5865\n",
            "Epoch 9, Train Loss: 0.0415, Train Accuracy: 0.9836, Val Loss: 1.3198, Val Accuracy: 0.5866\n",
            "Epoch 10, Train Loss: 0.0376, Train Accuracy: 0.9850, Val Loss: 1.2752, Val Accuracy: 0.5875\n",
            "Epoch 11, Train Loss: 0.0333, Train Accuracy: 0.9865, Val Loss: 1.4484, Val Accuracy: 0.5670\n",
            "Epoch 12, Train Loss: 0.0312, Train Accuracy: 0.9872, Val Loss: 1.2655, Val Accuracy: 0.5834\n",
            "Epoch 13, Train Loss: 0.0324, Train Accuracy: 0.9870, Val Loss: 1.2103, Val Accuracy: 0.6052\n",
            "Best model saved with validation accuracy: 0.6052\n",
            "Epoch 14, Train Loss: 0.0285, Train Accuracy: 0.9880, Val Loss: 1.4249, Val Accuracy: 0.5789\n",
            "Epoch 15, Train Loss: 0.0285, Train Accuracy: 0.9882, Val Loss: 1.2337, Val Accuracy: 0.5740\n",
            "Epoch 16, Train Loss: 0.0254, Train Accuracy: 0.9889, Val Loss: 1.4123, Val Accuracy: 0.5958\n",
            "Epoch 17, Train Loss: 0.0278, Train Accuracy: 0.9881, Val Loss: 1.3641, Val Accuracy: 0.5815\n",
            "Epoch 18, Train Loss: 0.0264, Train Accuracy: 0.9886, Val Loss: 1.4652, Val Accuracy: 0.5536\n",
            "Epoch 19, Train Loss: 0.0234, Train Accuracy: 0.9899, Val Loss: 1.4117, Val Accuracy: 0.5814\n",
            "Epoch 20, Train Loss: 0.0240, Train Accuracy: 0.9897, Val Loss: 1.3488, Val Accuracy: 0.5746\n",
            "Epoch 21, Train Loss: 0.0242, Train Accuracy: 0.9896, Val Loss: 1.4653, Val Accuracy: 0.5782\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = CNNLSTMModel(num_classes=7).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "model = train_model(model, train_loader, val_loader, criterion, optimizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "e4gMppfD-7hj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}