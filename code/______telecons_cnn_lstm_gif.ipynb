{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4184406c",
   "metadata": {},
   "source": [
    "#  <center> Telecons + DLmath (CNN+ LSTM) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe234c9",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad28f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1948b",
   "metadata": {},
   "source": [
    "## 2. Define the PyTorch Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2cbdc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정의\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, videos, labels, transform=None):\n",
    "        self.videos = videos\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video = self.videos[idx]  # (T, H, W, C)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 동영상 프레임 별로 전처리\n",
    "        if self.transform:\n",
    "            video = torch.stack([self.transform(frame) for frame in video])\n",
    "        \n",
    "        return video, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e0cda5",
   "metadata": {},
   "source": [
    "## 3. CNN-LSTM Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764a9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        \n",
    "        # CNN Layers\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 64 -> 16\n",
    "\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 16 -> 4\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 4 -> 1\n",
    "        )\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=32, hidden_size=128, num_layers=2, batch_first=True)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.shape # 4, 20, 3, 64, 64\n",
    "        x = x.view(batch_size * seq_len, c, h, w)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(batch_size, seq_len, -1)  # (batch_size, seq_len, feature_dim) # 4 x 20 x 32\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        x = self.fc(hidden[-1])  # 마지막 LSTM layer의 hidden state 사용\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8d9f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_videos_split_frames(base_dir, folders, frame_size=(64, 64), num_frames=5):\n",
    "    \"\"\"\n",
    "    동영상을 5등분하여 각 구간에서 1프레임씩 총 5프레임을 추출\n",
    "\n",
    "    Parameters:\n",
    "        base_dir (str): 동영상 폴더의 기본 디렉토리\n",
    "        folders (list): 하위 폴더 리스트\n",
    "        frame_size (tuple): 프레임 크기 (H, W)\n",
    "        num_frames (int): 추출할 프레임 수 (기본값 5)\n",
    "\n",
    "    Returns:\n",
    "        videos (list): 영상 데이터 리스트 [(T, H, W, C)]\n",
    "        labels (list): 라벨 리스트\n",
    "    \"\"\"\n",
    "    videos, labels = [], []\n",
    "    label_map = {\"C1\": 0, \"C2\": 1, \"C5\": 2}  # 클래스 라벨링\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith(\".gif\"):\n",
    "                    video_path = os.path.join(folder_path, file_name)\n",
    "                    cap = cv2.VideoCapture(video_path)\n",
    "                    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "                    if total_frames >= num_frames:\n",
    "                        step = total_frames // num_frames\n",
    "                        selected_frames = []\n",
    "\n",
    "                        for i in range(num_frames):\n",
    "                            frame_idx = i * step\n",
    "                            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "                            ret, frame = cap.read()\n",
    "                            if not ret:\n",
    "                                break\n",
    "                            frame = cv2.resize(frame, frame_size)\n",
    "                            selected_frames.append(frame)\n",
    "\n",
    "                        if len(selected_frames) == num_frames:\n",
    "                            videos.append(np.array(selected_frames))  # (T, H, W, C)\n",
    "                            labels.append(label_map[folder.split(\"_\")[0]])\n",
    "\n",
    "                    cap.release()\n",
    "    return videos, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd8af2",
   "metadata": {},
   "source": [
    "## 4. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d80bf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/work/DLmath/Seulbin/Telecons/data/thermal_processed/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m folders \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC1_KNG_0207\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC1_SHM_0201\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC2_KNG_0207\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC2_SHM_0201_0229\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC5_SHM_0229\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m videos, video_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_videos_split_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train-Val-Test 분할\u001b[39;00m\n\u001b[1;32m      8\u001b[0m train_videos, temp_videos, train_labels, temp_labels \u001b[38;5;241m=\u001b[39m train_test_split(videos, video_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m, in \u001b[0;36mload_videos_split_frames\u001b[0;34m(base_dir, folders, frame_size, num_frames)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_frames):\n\u001b[1;32m     36\u001b[0m     frame_idx \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m step\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCAP_PROP_POS_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 주요 파라미터 및 실행\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_dir = \"/home/work/DLmath/Seulbin/Telecons/data/thermal_processed/\"\n",
    "folders = [\"C1_KNG_0207\", \"C1_SHM_0201\", \"C2_KNG_0207\", \"C2_SHM_0201_0229\", \"C5_SHM_0229\"]\n",
    "videos, video_labels = load_videos_split_frames(base_dir, folders)\n",
    "\n",
    "# Train-Val-Test 분할\n",
    "train_videos, temp_videos, train_labels, temp_labels = train_test_split(videos, video_labels, test_size=0.4, random_state=42)\n",
    "val_videos, test_videos, val_labels, test_labels = train_test_split(temp_videos, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "train_dataset = VideoDataset(train_videos, train_labels, transform)\n",
    "val_dataset = VideoDataset(val_videos, val_labels, transform)\n",
    "test_dataset = VideoDataset(test_videos, test_labels, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ab8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_videos_split_frames(base_dir, folders, frame_size=(64, 64), num_frames=5):\n",
    "    \"\"\"\n",
    "    동영상을 5등분하여 각 구간에서 1프레임씩 총 5프레임을 추출\n",
    "\n",
    "    Parameters:\n",
    "        base_dir (str): 동영상 폴더의 기본 디렉토리\n",
    "        folders (list): 하위 폴더 리스트\n",
    "        frame_size (tuple): 프레임 크기 (H, W)\n",
    "        num_frames (int): 추출할 프레임 수 (기본값 5)\n",
    "\n",
    "    Returns:\n",
    "        videos (list): 영상 데이터 리스트 [(T, H, W, C)]\n",
    "        labels (list): 라벨 리스트\n",
    "    \"\"\"\n",
    "    videos, labels = [], []\n",
    "    label_map = {\"C1\": 0, \"C2\": 1, \"C5\": 2}  # 클래스 라벨링\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith(\".gif\"):\n",
    "                    video_path = os.path.join(folder_path, file_name)\n",
    "                    cap = cv2.VideoCapture(video_path)\n",
    "                    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "                    if total_frames >= num_frames:\n",
    "                        step = total_frames // num_frames\n",
    "                        selected_frames = []\n",
    "\n",
    "                        for i in range(num_frames):\n",
    "                            frame_idx = i * step\n",
    "                            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "                            ret, frame = cap.read()\n",
    "                            if not ret:\n",
    "                                break\n",
    "                            frame = cv2.resize(frame, frame_size)\n",
    "                            selected_frames.append(frame)\n",
    "\n",
    "                        if len(selected_frames) == num_frames:\n",
    "                            videos.append(np.array(selected_frames))  # (T, H, W, C)\n",
    "                            labels.append(label_map[folder.split(\"_\")[0]])\n",
    "\n",
    "                    cap.release()\n",
    "    return videos, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09060fe9",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=50):\n",
    "    best_model = None\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_preds, total_preds = 0, 0\n",
    "\n",
    "        for videos, labels in train_loader:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "\n",
    "        train_acc = correct_preds / total_preds\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_preds, total_preds = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for videos, labels in val_loader:\n",
    "                videos, labels = videos.to(device), labels.to(device)\n",
    "                outputs = model(videos)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # 정확도 계산\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                correct_preds += (preds == labels).sum().item()\n",
    "                total_preds += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct_preds / total_preds\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        # Best Model 저장\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = model.state_dict()\n",
    "            torch.save(best_model, \"best_model_cnn_lstm.pth\")\n",
    "            print(f\"Best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "    # 최종적으로 Best Model 로드\n",
    "    if best_model:\n",
    "        model.load_state_dict(best_model)\n",
    "        print(f\"Best model loaded with validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# 테스트 함수\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in test_loader:\n",
    "            videos = videos.to(device)\n",
    "            outputs = model(videos)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9998adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델, 손실 함수, 옵티마이저 정의\n",
    "model = CNNLSTMModel(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076f262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습 및 테스트\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a811000",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9536f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "PyTorch 2.2 (NGC 23.11/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
