{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4184406c",
   "metadata": {},
   "source": [
    "#  <center> Telecons + DLmath (CNN+ LSTM) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe234c9",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad28f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c661f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 폴더 경로\n",
    "data_folder = '/home/work/DLmath/Seulbin/Telecons/mydata/C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23adb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 통합 매핑\n",
    "label_map = {\n",
    "    \"C1\": \"Normal\",  # 정상\n",
    "    \"C3\": \"Normal\",  # 정상\n",
    "    \"C4\": \"Normal\",  # 정상\n",
    "    \"C2\": \"BedExit\", # 침대 이탈\n",
    "    \"C5\": \"BedExit\"  # 침대 이탈\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5467ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 저장용 리스트\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a68a9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(data_folder):\n",
    "    if file_name.endswith(('.mp4', '.avi', '.mkv')):  # 비디오 파일만 처리\n",
    "        # 원본 레이블 추출\n",
    "        original_label = file_name.split('_')[0]  # 파일 이름의 첫 번째 부분을 레이블로 사용\n",
    "        \n",
    "        # 통합 레이블 매핑\n",
    "        if original_label in label_map:\n",
    "            unified_label = label_map[original_label]\n",
    "            labels.append({\n",
    "                'FilePath': os.path.join(data_folder, file_name),\n",
    "                'Label': unified_label\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4363f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임으로 변환 후 CSV로 저장\n",
    "df = pd.DataFrame(labels)\n",
    "df.to_csv('video_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1948b",
   "metadata": {},
   "source": [
    "## 2. Define the PyTorch Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2cbdc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, csv_file, num_frames=6, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.label_map = {label: idx for idx, label in enumerate(self.data['Label'].unique())}\n",
    "\n",
    "    def extract_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        step = max(1, total_frames // self.num_frames)\n",
    "        \n",
    "        for i in range(self.num_frames):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.resize(frame, (256, 256))\n",
    "                frame = frame / 255.0  # Normalize\n",
    "                if self.transform:\n",
    "                    frame = self.transform(frame)\n",
    "                frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "        # 프레임 수가 부족한 경우 마지막 프레임 복제 또는 검정 화면 추가\n",
    "        while len(frames) < self.num_frames:\n",
    "            if len(frames) > 0:\n",
    "                frames.append(frames[-1])  # 마지막 프레임 복제\n",
    "            else:\n",
    "                frames.append(np.zeros((256, 256, 3), dtype=np.float32))  # 검정 화면 추가\n",
    "\n",
    "        # 초과된 프레임은 자르기\n",
    "        frames = frames[:self.num_frames]\n",
    "        \n",
    "        return torch.stack(frames)  # Tensor로 반환\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.data.iloc[idx]['FilePath']\n",
    "        label = self.data.iloc[idx]['Label']\n",
    "        frames = self.extract_frames(video_path)\n",
    "        label_idx = self.label_map[label]\n",
    "        return frames.permute(0, 3, 1, 2), torch.tensor(label_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e0cda5",
   "metadata": {},
   "source": [
    "## 3. CNN-LSTM Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ab9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN+LSTM 모델\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time_steps, channels, height, width)\n",
    "        batch_size, time_steps, c, h, w = x.size()\n",
    "        x = x.view(-1, c, h, w)  # 배치와 타임스텝 결합\n",
    "        x = self.module(x)            \n",
    "            \n",
    "        out_c, out_h, out_w = x.size(1), x.size(2), x.size(3)\n",
    "        x = x.view(batch_size, time_steps, out_c, out_h, out_w)  # 배치와 타임스텝 복원\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        \n",
    "        # CNN Layers\n",
    "        self.cnn = nn.Sequential(\n",
    "            TimeDistributed(nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)),\n",
    "            TimeDistributed(nn.BatchNorm2d(16)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(4)), # 256 -> 64\n",
    "            TimeDistributed(nn.Dropout(0.3)),\n",
    "            \n",
    "            TimeDistributed(nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)),\n",
    "            TimeDistributed(nn.BatchNorm2d(32)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(4)), # 56 -> 16\n",
    "            TimeDistributed(nn.Dropout(0.3)),\n",
    "            \n",
    "            TimeDistributed(nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)),\n",
    "            TimeDistributed(nn.BatchNorm2d(64)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(4)), # 16 -> 4\n",
    "            TimeDistributed(nn.Dropout(0.3)),\n",
    "        )\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=64*4*4, hidden_size=32, batch_first=True) \n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, c, h, w = x.size()\n",
    "        x = x.permute(0, 1, 3, 2, 4)\n",
    "        x = self.cnn(x) #CNN Forward # Shape: (batch_size, time_steps, 64, 7, 7)\n",
    "        x = x.contiguous().view(batch_size, time_steps, -1)  # Shape: (batch_size, time_steps, 64*7*7)\n",
    "        _, (hidden, _) = self.lstm(x) # Shape: hidden: (1, batch_size, 16)\n",
    "        x = hidden[-1]  # Shape: (batch_size, 16)\n",
    "        x = self.fc(x) # Shape: (batch_size, num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e54008c",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7164f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "            patience (int): 성능이 개선되지 않을 때 기다리는 epoch 수.\n",
    "            verbose (bool): True일 경우, 성능 개선 시 메시지를 출력.\n",
    "            delta (float): 성능 향상으로 간주할 최소 변화량.\n",
    "            path (str): 체크포인트를 저장할 경로.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"검증 손실이 감소할 때 모델을 저장합니다.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model...\")\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3e289",
   "metadata": {},
   "source": [
    "## 4. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27d80bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 데이터 분할\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data.to_csv('train_labels.csv', index=False)\n",
    "val_data.to_csv('val_labels.csv', index=False)\n",
    "\n",
    "# 데이터셋 초기화\n",
    "train_dataset = VideoDataset(csv_file='train_labels.csv')\n",
    "val_dataset = VideoDataset(csv_file='val_labels.csv')\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # 배치 크기를 4로 설정\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)  # 검증 데이터는 섞지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09060fe9",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cc2ee05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     44\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frames, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     47\u001b[0m     frames, labels \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     49\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m, in \u001b[0;36mVideoDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     41\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFilePath\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     42\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 43\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m label_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_map[label]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frames\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor(label_idx)\n",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m, in \u001b[0;36mVideoDataset.extract_frames\u001b[0;34m(self, video_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 초과된 프레임은 자르기\u001b[39;00m\n\u001b[1;32m     33\u001b[0m frames \u001b[38;5;241m=\u001b[39m frames[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_frames]\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(train_data['Label']), \n",
    "    y=df['Label']\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "num_classes = len(df['Label'].unique())\n",
    "model = CNNLSTMModel(num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "# EarlyStopping 객체 생성\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True, path='best_model.pt')\n",
    "\n",
    "# Scheduler 정의\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# 학습 및 검증 손실/정확도 기록용 리스트\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for frames, labels in train_loader:\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    train_accuracies.append(100. * correct / total)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss/len(train_loader)}, Accuracy: {100.*correct/total}%\")\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, labels in val_loader:\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accuracies.append(100. * correct / total)\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100.*correct/total}%\")\n",
    "    \n",
    "    # 학습률 스케줄러 업데이트\n",
    "    scheduler.step(val_loss / len(val_loader))\n",
    "\n",
    "    # EarlyStopping 호출\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Early Stopping 이후 최적 모델 로드\n",
    "if early_stopping.early_stop:\n",
    "    model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e6c31",
   "metadata": {},
   "source": [
    "## save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d064b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# Validation Loss와 Accuracy 계산\n",
    "val_loss_avg = val_loss / len(val_loader)\n",
    "val_accuracy = 100. * correct / total\n",
    "\n",
    "# 현재 날짜 및 시간 포맷 지정\n",
    "date_time_format = '%Y_%m_%d__%H_%M_%S'\n",
    "current_date_time = dt.datetime.now().strftime(date_time_format)\n",
    "\n",
    "# 파일 이름 정의\n",
    "model_file_name = f\"cnn_lstm_model__Date_Time_{current_date_time}__Loss_{val_loss_avg:.4f}__Accuracy_{val_accuracy:.2f}.pth\"\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), model_file_name)\n",
    "print(f\"Model saved as {model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7136d9",
   "metadata": {},
   "source": [
    "## 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca09dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(train_values, val_values, metric_name):\n",
    "    \"\"\"\n",
    "        train_values: 훈련 데이터 값 리스트\n",
    "        val_values: 검증 데이터 값 리스트\n",
    "        metric_name: 지표 이름 (e.g., \"Loss\", \"Accuracy\")\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_values) + 1)\n",
    "    plt.plot(epochs, train_values, 'b-', label=f'Train {metric_name}')\n",
    "    plt.plot(epochs, val_values, 'r-', label=f'Validation {metric_name}')\n",
    "    plt.title(f'Train and Validation {metric_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 시각화\n",
    "plot_metrics(train_losses, val_losses, \"Loss\")\n",
    "\n",
    "# 정확도 시각화\n",
    "plot_metrics(train_accuracies, val_accuracies, \"Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "PyTorch 2.2 (NGC 23.11/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
